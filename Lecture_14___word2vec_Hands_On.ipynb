{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GuilhermePascon/ml/blob/main/Lecture_14___word2vec_Hands_On.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCBouAgYen2S"
      },
      "source": [
        "###  MC959-MO810 -- Introduction to Self-Supervised (Representation) Learning (SSRL)\n",
        "\n",
        "* Instructor: Marcelo S. Reis. <a href=\"mailto:msreis@unicamp.br\">msreis@unicamp.br</a>\n",
        "\n",
        "Campinas, September 30, 2024.\n",
        "\n",
        "\n",
        "## Lecture 14: word2vec Hands On\n",
        "\n",
        "*Based on the [word2vec-pytorch](https://github.com/OlgaChernytska/word2vec-pytorch) implementation by Chernytska.*\n",
        "\n",
        "In this lab, we will carry out a SSRL pipeline using the classic word2vec method. The original word2vec paper proposes two simple models that use as pretext task either the center word prediction (bag-of-words) or the neighbor word prediction (skip-gram). In skip-gram, we feed a model with a pair $(X,Z)$ of words, where $Z = w(i)$ is a neighbor word of $X = w(t)$ (either one of the $w(t-N), \\ldots, w(t-1)$ words in the history or one of the $w(t+1), \\ldots, w(t+N)$ words in the future) and the pretext task is the prediction what are the neighbor words of a given $X$. An example for $k = 2$:\n",
        "\n",
        "<img src=\"http://www.ic.unicamp.br/~msreis/Skip-gram.png\">\n",
        "\n",
        "In this lab, we'll implement in PyTorch the word2vec model using the ski-gram pretext task. We'll fully pretrain that model, visualize the yielded embeddings with a dimensionality reduction technique called t-SNE and also verify the relationship between words through algebraic operations between vectors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eoj0IwtEen2X"
      },
      "source": [
        "### Summary <a class=\"anchor\" id=\"topo\"></a>\n",
        "\n",
        "* [Part 1: Solving main dependencies](#part_01).\n",
        "* [Part 2: Setting up the pipeline](#part_02).\n",
        "* [Part 3: Loading the WikiText-2 dataset and preprocessing text](#part_03).\n",
        "* [Part 4: Implementing the word2vec skip-gram model and trainer](#part_04).\n",
        "* [Part 5: Pretraining of our word2vec skip-gram model](#part_05).\n",
        "* [Part 6: Visualizing the yielded embeddings with t-SNE](#part_06).\n",
        "* [Part 7: Algebraic checking of similar words](#part_07).\n",
        "* [Part 8: Suggestion of exercises on this pipeline](#part_08).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnH8HlRhen2Z"
      },
      "source": [
        "### Part 1: Solving main dependencies <a class=\"anchor\" id=\"part_01\"></a>\n",
        "\n",
        "Here we load the main libraries that will be used throughout this notebook.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MaTT0lUzen2a"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Install (if needed) and import torchtext, a package for NLP.\n",
        "#\n",
        "# https://pytorch.org/text/stable/index.html\n",
        "# https://github.com/pytorch/text#installation\n",
        "#\n",
        "#!pip install torch==2.3.0\n",
        "#!pip install torchtext==0.18.0\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F    # to import the cosine similarity and cross entropy\n",
        "import torch.optim as optim\n",
        "import torchtext\n",
        "\n",
        "print(torch.__version__)\n",
        "print(torchtext.__version__)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H39nT5LL0bF2"
      },
      "source": [
        "### Part 2: Setting up the pipeline <a class=\"anchor\" id=\"part_02\"></a>\n",
        "\n",
        "Initialization of pseudo-random number generator seed, paths, and so forth.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGT_JEt30bF3",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Global seed (useful for reproducibility of our pipeline)\n",
        "#\n",
        "torch.manual_seed(46)\n",
        "\n",
        "# The following three variables are hyperparameters of the\n",
        "# pretext task.\n",
        "\n",
        "# Dimension of the embeddings (word vectors).\n",
        "#\n",
        "EMBED_DIMENSION = 300\n",
        "\n",
        "# Minimum frequency to allow a word to be included into vocabulary.\n",
        "#\n",
        "MIN_WORD_FREQUENCY = 50\n",
        "\n",
        "# Skip-gram window size.\n",
        "#\n",
        "SKIPGRAM_N_WORDS = 4\n",
        "\n",
        "\n",
        "# Maximum number of tokens in a paragraph.\n",
        "#\n",
        "MAX_SEQUENCE_LENGTH = 256\n",
        "\n",
        "\n",
        "# Max value of the Euclidean (L2) norm applied at any embedding.\n",
        "# Setting here with a real value instead of \"None\" acts as a\n",
        "# regularization parameter of the pretraining procedure.\n",
        "#\n",
        "EMBED_MAX_NORM = 1\n",
        "\n",
        "# Path to the folder where the datasets are/should be downloaded (e.g. )\n",
        "#\n",
        "DATASET_PATH = \"./\"\n",
        "\n",
        "# Path to the folder where the pretrained models are saved.\n",
        "#\n",
        "CHECKPOINT_PATH = \"./\"\n",
        "\n",
        "# Checking the number of CPU cores\n",
        "#\n",
        "import os\n",
        "NUM_WORKERS = os.cpu_count()\n",
        "\n",
        "# Ensure that all operations are deterministic on GPU (if used) for reproducibility.\n",
        "#\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"Device:\", device)\n",
        "print(\"Number of workers:\", NUM_WORKERS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XV0KCBasqGmW"
      },
      "source": [
        "### Part 3: Loading the WikiText-2 dataset and preprocessing text <a class=\"anchor\" id=\"part_03\"></a>\n",
        "\n",
        "The WikiText-2 is a small text dataset available at Pytorch (~ 2M of tokens, ~ 36K lines). This dataset is composed of a set of verified good and feature articles from English Wikipedia.\n",
        "\n",
        "In this step, we'll also define some preprocessing functions: one that sets the tokenizer (token extractor from text), one for setting the vocabulary, one to get the dataset iterator, and one to define a collage_fn (a function to dynamically extract batches of different paragraph sizes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7CE3lklkrJQ7"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "from torch.utils.data import DataLoader\n",
        "from torchtext.data import to_map_style_dataset\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.datasets import WikiText2\n",
        "\n",
        "\n",
        "def get_english_tokenizer():\n",
        "    \"\"\"\n",
        "    Basic English tokenizer lowercases text and splits tokens\n",
        "    by whitespaces. Punctuations are also included as tokens.\n",
        "\n",
        "    For example, the sentence: I like Earth!\n",
        "\n",
        "    Generates the tokens \"i\", \"like\", \"earth\" and \"!\".\n",
        "    \"\"\"\n",
        "    tokenizer = get_tokenizer(\"basic_english\", language=\"en\")\n",
        "    return tokenizer\n",
        "\n",
        "\n",
        "def get_data_iterator(ds_type, data_dir):\n",
        "    \"\"\"\n",
        "    Receives a dataset type (train, valid or test) and the dataset directory,\n",
        "    and returns a map data iterator for the dataset in the directory.\n",
        "    \"\"\"\n",
        "    data_iter = WikiText2(root=data_dir, split=(ds_type))\n",
        "    data_iter = to_map_style_dataset(data_iter)\n",
        "    return data_iter\n",
        "\n",
        "\n",
        "def build_vocab(data_iter, tokenizer):\n",
        "    \"\"\"\n",
        "    This function returns a vocabulary in which it will be\n",
        "    included only words that appear at least MIN_WORD_FREQUENCY\n",
        "    in the dataset.\n",
        "\n",
        "    For a vocabulary of size N, each word included into it will\n",
        "    have an ID ranging from 1 to N.\n",
        "\n",
        "    Every word with frequency < 50 will encode with ID = 0.\n",
        "    \"\"\"\n",
        "    vocab = build_vocab_from_iterator(\n",
        "        map(tokenizer, data_iter),\n",
        "        specials=[\"<unk>\"],          # In WikiText2, rare words are signed with the token <unk>\n",
        "        min_freq=MIN_WORD_FREQUENCY, # We'll set that token as a special symbol (ID = 0).\n",
        "    )\n",
        "    vocab.set_default_index(vocab[\"<unk>\"])\n",
        "    return vocab\n",
        "\n",
        "\n",
        "def collate_skipgram(batch, text_pipeline):\n",
        "    \"\"\"\n",
        "    Creates a customized collate_fn (the batch creator function)\n",
        "    for the Dataloader.\n",
        "\n",
        "    \"batch\" is expected to be list of text paragrahs.\n",
        "\n",
        "    Context is represented as N=SKIPGRAM_N_WORDS past words\n",
        "    and N=SKIPGRAM_N_WORDS future words.\n",
        "\n",
        "    Long paragraphs will be truncated to contain\n",
        "    no more that MAX_SEQUENCE_LENGTH tokens.\n",
        "\n",
        "    For a tutorial on modifing collate_fn, refer to:\n",
        "\n",
        "    https://python.plainenglish.io/understanding-collate-fn-in-pytorch-f9d1742647d3\n",
        "    \"\"\"\n",
        "    batch_input  = []  # each element of this list is a middle word (\"X\").\n",
        "    batch_output = []  # each element of this list is a context word (\"Z\").\n",
        "\n",
        "    for paragraph in batch:\n",
        "\n",
        "        text_tokens_ids = text_pipeline(paragraph)\n",
        "\n",
        "        N = SKIPGRAM_N_WORDS\n",
        "\n",
        "        # Paragraph too short (can't take even a single skipgram); skip it.\n",
        "        #\n",
        "        if len(text_tokens_ids) < 2 * N + 1:\n",
        "            continue\n",
        "\n",
        "        # Paragraph too long; truncate it.\n",
        "        #\n",
        "        elif len(text_tokens_ids) > MAX_SEQUENCE_LENGTH:\n",
        "            text_tokens_ids = text_tokens_ids[:MAX_SEQUENCE_LENGTH]\n",
        "\n",
        "        # With the moving window of size 2N+1 (N history words, middle word,\n",
        "        # and N future words) it loops through the paragraph.\n",
        "        #\n",
        "        for idx in range(len(text_tokens_ids) - 2 * N):\n",
        "\n",
        "            # Takes a token subset that starts at position idx.\n",
        "            #\n",
        "            token_id_sequence = text_tokens_ids[idx : (idx + 2 * N + 1)]\n",
        "\n",
        "            # Remove the element index N (X, middle word)\n",
        "            #\n",
        "            # For N == 4:\n",
        "            #\n",
        "            # index:     0  1  2  3  4  5  6  7  8\n",
        "            #            z  z  z  z  x  z  z  z  z\n",
        "            #\n",
        "            input_ = token_id_sequence.pop(N)\n",
        "\n",
        "            # Keep elements 0..N-1 and N+1..2N (Z, history and future words)\n",
        "            #\n",
        "            outputs = token_id_sequence\n",
        "\n",
        "            # Create 2N pairs (X,Z) in the form:\n",
        "            #\n",
        "            #  (input_, output_1), (input_, output_2), ..., (input, output_2N)\n",
        "            #\n",
        "            for output in outputs:\n",
        "                batch_input.append(input_)\n",
        "                batch_output.append(output)\n",
        "\n",
        "    # Merge middle words of all paragraphs (\"Xs\")\n",
        "    #\n",
        "    batch_input = torch.tensor(batch_input, dtype=torch.long)\n",
        "\n",
        "    # Merge context words of all paragraphs (\"Zs\")\n",
        "    #\n",
        "    batch_output = torch.tensor(batch_output, dtype=torch.long)\n",
        "\n",
        "    return batch_input, batch_output\n",
        "\n",
        "\n",
        "def get_dataloader_and_vocab(ds_type, data_dir, batch_size, shuffle, vocab=None):\n",
        "    \"\"\"\n",
        "    Receives the dataset type, the data directory, batch size and a shuffle flag.\n",
        "    It gets the data iterator and the tokenizer, create a vocabulary if the vocab\n",
        "    argument doesn't contain one and use all of that to create and return a dataloader.\n",
        "    \"\"\"\n",
        "    data_iter = get_data_iterator(ds_type, data_dir)\n",
        "    tokenizer = get_english_tokenizer()\n",
        "\n",
        "    if not vocab:\n",
        "        vocab = build_vocab(data_iter, tokenizer)\n",
        "\n",
        "    text_pipeline = lambda x: vocab(tokenizer(x))\n",
        "\n",
        "    dataloader = DataLoader(\n",
        "        data_iter,\n",
        "        batch_size = batch_size,\n",
        "        shuffle = shuffle,\n",
        "        collate_fn = partial(collate_skipgram, text_pipeline = text_pipeline),\n",
        "    )\n",
        "\n",
        "    return dataloader, vocab\n",
        "\n",
        "\n",
        "print(\"All functions sucessfully declared.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2PXZUrcyvXR"
      },
      "source": [
        "### Part 4: Implementing the word2vec skip-gram model and trainer<a class=\"anchor\" id=\"part_04\"></a>\n",
        "\n",
        "Here we will implement two classes: <code>SkipGram_Model</code> for the word2vec skip-gram model architecture; and <code>Trainer</code>, for a trainer.\n",
        "\n",
        "<code>SkipGram_Model</code> is a class that uses nn.Module, a base class for all neural network modules.\n",
        "\n",
        "<code>Trainer</code> is a class that at instantiation initializes a model, the data loader and optimization parameters. It also contains methods for training, validation and saving the yielded results (checkpoints, model, loss)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rduoTMQGyvfH"
      },
      "outputs": [],
      "source": [
        "# For the class Trainer.\n",
        "#\n",
        "import json\n",
        "\n",
        "class SkipGram_Model(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementation of Skip-Gram model described in the word2vec paper.\n",
        "    https://arxiv.org/abs/1301.3781\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size: int):\n",
        "        super(SkipGram_Model, self).__init__()\n",
        "\n",
        "        # A simple lookup table that stores embeddings of\n",
        "        # a fixed dictionary and size.\n",
        "        #\n",
        "        self.embeddings = nn.Embedding(\n",
        "            num_embeddings = vocab_size,\n",
        "            embedding_dim = EMBED_DIMENSION,\n",
        "            max_norm = EMBED_MAX_NORM)\n",
        "\n",
        "        # The model itself.\n",
        "        #\n",
        "        self.linear = nn.Linear(\n",
        "            in_features = EMBED_DIMENSION,\n",
        "            out_features = vocab_size)\n",
        "\n",
        "    def forward(self, inputs_):\n",
        "        x = self.embeddings(inputs_)  # Inputs_ yields projection\n",
        "        x = self.linear(x)            # Projection yields output\n",
        "\n",
        "        # Output has no softmax: Pytorch' CrossEntropyLoss uses raw scores.\n",
        "        #\n",
        "        return x\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    \"\"\"\n",
        "    Main class for model training. It is implemented as\n",
        "    a typical PyTorch train and validation flow.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        model,\n",
        "        epochs,\n",
        "        train_dataloader,\n",
        "        val_dataloader,\n",
        "        criterion,\n",
        "        optimizer,\n",
        "        lr_scheduler,\n",
        "        device,\n",
        "        model_dir,\n",
        "        model_name,\n",
        "    ):\n",
        "        self.model = model\n",
        "        self.epochs = epochs\n",
        "        self.train_dataloader = train_dataloader\n",
        "        self.val_dataloader = val_dataloader\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "        self.lr_scheduler = lr_scheduler\n",
        "        self.device = device\n",
        "        self.model_dir = model_dir\n",
        "        self.model_name = model_name\n",
        "\n",
        "        self.loss = {\"train\": [], \"val\": []}\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def train(self):\n",
        "        for epoch in range(self.epochs):\n",
        "            self._train_epoch()\n",
        "            self._validate_epoch()\n",
        "            print(\n",
        "                \"Epoch: {}/{}, Train Loss={:.5f}, Val Loss={:.5f}\".format(\n",
        "                    epoch + 1,\n",
        "                    self.epochs,\n",
        "                    self.loss[\"train\"][-1],\n",
        "                    self.loss[\"val\"][-1],\n",
        "                )\n",
        "            )\n",
        "\n",
        "            self.lr_scheduler.step()\n",
        "\n",
        "            self._save_checkpoint(epoch)\n",
        "\n",
        "    def _train_epoch(self):\n",
        "        \"\"\"\n",
        "        Run a training epoch, recording the updated loss at each step.\n",
        "        \"\"\"\n",
        "        self.model.train()\n",
        "        running_loss = []\n",
        "\n",
        "        for i, batch_data in enumerate(self.train_dataloader, 1):\n",
        "            inputs = batch_data[0].to(self.device)\n",
        "            labels = batch_data[1].to(self.device)\n",
        "\n",
        "            self.optimizer.zero_grad()      # Resets the gradients of all optimized tensors.\n",
        "            outputs = self.model(inputs)\n",
        "            loss = self.criterion(outputs, labels)\n",
        "            loss.backward()                        # Compute the gradient.\n",
        "            self.optimizer.step()\n",
        "\n",
        "            running_loss.append(loss.item())\n",
        "\n",
        "        epoch_loss = np.mean(running_loss)\n",
        "        self.loss[\"train\"].append(epoch_loss)\n",
        "\n",
        "    def _validate_epoch(self):\n",
        "        \"\"\"\n",
        "        Run a validation epoch, recording the updated loss at each step.\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "        running_loss = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i, batch_data in enumerate(self.val_dataloader, 1):\n",
        "                inputs = batch_data[0].to(self.device)\n",
        "                labels = batch_data[1].to(self.device)\n",
        "\n",
        "                outputs = self.model(inputs)\n",
        "                loss = self.criterion(outputs, labels)\n",
        "\n",
        "                running_loss.append(loss.item())\n",
        "\n",
        "        epoch_loss = np.mean(running_loss)\n",
        "        self.loss[\"val\"].append(epoch_loss)\n",
        "\n",
        "    def _save_checkpoint(self, epoch):\n",
        "        \"\"\"\n",
        "        Save model checkpoint to `self.model_dir` directory.\n",
        "        \"\"\"\n",
        "        epoch_num = epoch + 1\n",
        "        model_path = \"checkpoint_{}.pt\".format(str(epoch_num).zfill(3))\n",
        "        model_path = os.path.join(self.model_dir, model_path)\n",
        "        torch.save(self.model, model_path)\n",
        "\n",
        "    def save_model(self):\n",
        "        \"\"\"\n",
        "        Save final model to `self.model_dir` directory.\n",
        "        \"\"\"\n",
        "        model_path = os.path.join(self.model_dir, \"model.pt\")\n",
        "        torch.save(self.model, model_path)\n",
        "\n",
        "    def save_loss(self):\n",
        "        \"\"\"\n",
        "        Save train/val loss as json file to `self.model_dir` directory.\n",
        "        \"\"\"\n",
        "        loss_path = os.path.join(self.model_dir, \"loss.json\")\n",
        "        with open(loss_path, \"w\") as fp:\n",
        "            json.dump(self.loss, fp)\n",
        "\n",
        "print(\"word2vec skip-gram model and trainer sucessfully implemented.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKU0S7qVIU3u"
      },
      "source": [
        "### Part 5: Pretraining of our word2vec skip-gram model <a class=\"anchor\" id=\"part_05\"></a>\n",
        "\n",
        "Training for 5 epochs on CPUs (eight Intel i5-8265U CPU cores at 1.60GHz) took around 1 hour. With a good GPU this time can be reduced to less than 20 minutes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwkh1V_tIVCF"
      },
      "outputs": [],
      "source": [
        "#!pip install torchdata\n",
        "#!pip install portalocker>=2.0.0\n",
        "import torchdata\n",
        "\n",
        "def get_lr_scheduler(optimizer, total_epochs: int, verbose: bool = True):\n",
        "    \"\"\"\n",
        "      Scheduler to linearly decrease learning rate,\n",
        "      so that learning rate after the last epoch is 0.\n",
        "    \"\"\"\n",
        "    lr_lambda = lambda epoch: (total_epochs - epoch) / total_epochs\n",
        "    lr_scheduler = LambdaLR(optimizer, lr_lambda = lr_lambda, verbose = verbose)\n",
        "    return lr_scheduler\n",
        "\n",
        "\n",
        "def save_vocab(vocab, model_dir: str):\n",
        "    \"\"\"\n",
        "      Save vocab file to `model_dir` directory.\n",
        "    \"\"\"\n",
        "    vocab_path = os.path.join(model_dir, \"vocab.pt\")\n",
        "    torch.save(vocab, vocab_path)\n",
        "\n",
        "\n",
        "train_dataloader, vocab = get_dataloader_and_vocab(\n",
        "    ds_type = \"train\",\n",
        "    data_dir = DATASET_PATH,\n",
        "    batch_size = 96,\n",
        "    shuffle = True,\n",
        "    vocab = None)\n",
        "\n",
        "vocab_size = len(vocab.get_stoi())\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "\n",
        "save_vocab(vocab, CHECKPOINT_PATH)\n",
        "print(\"Model vocabulary saved to folder:\", CHECKPOINT_PATH)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYVQ7bS6NzTI"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "\n",
        "# We ignore the validation's vocab, since we already\n",
        "# got it from the training dataset.\n",
        "#\n",
        "val_dataloader, _ = get_dataloader_and_vocab(\n",
        "    ds_type = \"valid\",\n",
        "    data_dir = DATASET_PATH,\n",
        "    batch_size = 96,\n",
        "    shuffle = True,\n",
        "    vocab = vocab)\n",
        "\n",
        "model = SkipGram_Model(vocab_size = vocab_size)\n",
        "\n",
        "learning_rate = 0.025\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
        "\n",
        "n_epochs = 1\n",
        "\n",
        "trainer = Trainer(\n",
        "    model = model,\n",
        "    epochs = n_epochs,\n",
        "    train_dataloader = train_dataloader,\n",
        "    val_dataloader = val_dataloader,\n",
        "    criterion = nn.CrossEntropyLoss(),\n",
        "    optimizer = optimizer,\n",
        "    lr_scheduler = get_lr_scheduler(optimizer, n_epochs),\n",
        "    device = device,\n",
        "    model_dir = CHECKPOINT_PATH,\n",
        "    model_name = 'skipgram',\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "print(\"Training finished.\")\n",
        "\n",
        "trainer.save_model()\n",
        "trainer.save_loss()\n",
        "print(\"Model files saved to folder:\", CHECKPOINT_PATH)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRXD4h2hPa1c"
      },
      "source": [
        "### Part 6: Visualizing the yielded embeddings with t-SNE<a class=\"anchor\" id=\"part_06\"></a>\n",
        "\n",
        "The produced word vectors have EMBED_DIMENSION (300 in the execution for the lecture) size each. To have a full graphical view of our results is not possible for more than 3 dimensions. Therefore, we need to apply a dimensionality reduction technique.\n",
        "\n",
        "To this end, we will employ t-SNE, a dimensionality reduction technique that will create a non-linear projection of datapoints in the EMBED_DIMENSION space into a 2D space, whose points we can plot into a graph.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zN-f6ElhPa_C"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.manifold import TSNE\n",
        "!pip install plotly\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Getting the embeddings (from the first model layer).\n",
        "#\n",
        "# Each embedding is a 1D vector with EMBED_DIMENSION size.\n",
        "#\n",
        "# Hence all embeddings are a matrix of size vocab size x EMBED_DIMENSION.\n",
        "#\n",
        "embeddings = list(model.parameters())[0]\n",
        "embeddings = embeddings.cpu().detach().numpy() # convert from tensor to numpy\n",
        "\n",
        "# Normalization of the embeddings.\n",
        "#\n",
        "norms = (embeddings ** 2).sum(axis=1) ** (1 / 2)\n",
        "norms = np.reshape(norms, (len(norms), 1))\n",
        "embeddings_norm = embeddings / norms\n",
        "embeddings_norm.shape\n",
        "\n",
        "# Get the embeddings into a dataframe.\n",
        "#\n",
        "embeddings_df = pd.DataFrame(embeddings)\n",
        "\n",
        "# t-SNE projection.\n",
        "#\n",
        "tsne = TSNE(n_components=2)\n",
        "embeddings_df_trans = tsne.fit_transform(embeddings_df)\n",
        "embeddings_df_trans = pd.DataFrame(embeddings_df_trans)\n",
        "\n",
        "# Getting the token order.\n",
        "#\n",
        "embeddings_df_trans.index = vocab.get_itos()\n",
        "\n",
        "# In the case the token is a number.\n",
        "#\n",
        "is_numeric = embeddings_df_trans.index.str.isnumeric()\n",
        "\n",
        "color = np.where(is_numeric, \"green\", \"black\")\n",
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(\n",
        "    go.Scatter(\n",
        "        x=embeddings_df_trans[0],\n",
        "        y=embeddings_df_trans[1],\n",
        "        mode=\"text\",\n",
        "        text=embeddings_df_trans.index,\n",
        "        textposition=\"middle center\",\n",
        "        textfont=dict(color=color)))\n",
        "\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgD3xqNMRoqX"
      },
      "source": [
        "### Part 7: Algebraic checking of similar words<a class=\"anchor\" id=\"part_07\"></a>\n",
        "\n",
        "One interesting point of word embedding vectors yielded by word2vec is that, using a similarity distance such as the cosine similarity for a pairwise comparison of vectors, words related to each other should be more similar than words that are unrelated to each other\n",
        "\n",
        "Two classical examples are \"King - Man + Woman = Queen\" and \"Paris - France + Germany = Berlin\".\n",
        "\n",
        "Let us try now those algebraic operations and see what happens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5BtX1CoNzTJ"
      },
      "outputs": [],
      "source": [
        "def get_top_similar(word: str, N: int = 10):\n",
        "    word_id = vocab[word]\n",
        "    if word_id == 0:\n",
        "        print(\"Out of vocabulary word\")\n",
        "        return\n",
        "\n",
        "    word_vec = embeddings_norm[word_id]\n",
        "    word_vec = np.reshape(word_vec, (len(word_vec), 1))\n",
        "    dists = np.matmul(embeddings_norm, word_vec).flatten()\n",
        "    topN_ids = np.argsort(-dists)[1 : N + 1]\n",
        "\n",
        "    topN_dict = {}\n",
        "    for sim_word_id in topN_ids:\n",
        "        sim_word = vocab.lookup_token(sim_word_id)\n",
        "        topN_dict[sim_word] = dists[sim_word_id]\n",
        "    return topN_dict\n",
        "\n",
        "\n",
        "print(\"The five most similar words to the word 'Germany' are:\\n\")\n",
        "\n",
        "for word, sim in get_top_similar(\"germany\", 5).items():\n",
        "    print(\"{}: {:.3f}\".format(word, sim))\n",
        "\n",
        "#----------------------------------------------------------#\n",
        "\n",
        "emb1 = embeddings[vocab[\"king\"]]\n",
        "emb2 = embeddings[vocab[\"man\"]]\n",
        "emb3 = embeddings[vocab[\"woman\"]]\n",
        "\n",
        "emb4 = emb1 - emb2 + emb3\n",
        "\n",
        "# Calculate the Euclidean norm.\n",
        "#\n",
        "emb4_norm = (emb4 ** 2).sum() ** (1 / 2)\n",
        "emb4 = emb4 / emb4_norm\n",
        "\n",
        "emb4 = np.reshape(emb4, (len(emb4), 1))\n",
        "dists = np.matmul(embeddings_norm, emb4).flatten()\n",
        "\n",
        "print(\"\\n\\nThe five most similar words to 'King' - 'Man' + 'Woman' are:\\n\")\n",
        "\n",
        "closest_five_words = np.argsort(-dists)[:5]\n",
        "for word_id in closest_five_words:\n",
        "    print(\"{}: {:.3f}\".format(vocab.lookup_token(word_id), dists[word_id]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1adkYkCcNzTJ"
      },
      "source": [
        "### Part 8: Suggestions of exercises on this pipeline<a class=\"anchor\" id=\"part_08\"></a>\n",
        "\n",
        "Some possibilities for further explore the SSL pipeline presented here include:\n",
        "\n",
        "\n",
        "* Repeat the pipeline using the [WikiText103](https://pytorch.org/text/stable/_modules/torchtext/datasets/wikitext103.html) dataset instead of WikiText-2 (~ 100M tokens, 1.8M lines). Training on that dataset for 5 epochs should take an overnight in a GPU server.\n",
        "\n",
        "\n",
        "* Use the yielded embeddings for a downstream task. One suggestion is to try the [large movie review dataset](https://ai.stanford.edu/~amaas/data/sentiment/), which performs binary sentiment classification. It is a high-quality labeled dataset with 25K reviews for training and 25K reviews for testing. For this exercise use a simple linear model such as the logistic regression.\n",
        "\n",
        "\n",
        "* As a baseline for the previous exercise, try to carry out supervised learning of logistic regression directly on the large movie review dataset and compare the results. Did the usage of embeddings in the previous exercise yielded better results than here?\n",
        "\n",
        "\n",
        "* Instead of the skip-gram pretext task of the original word2vec paper, we can also try the skip-gram with negative sampling pretext task presented in Lecture 13, whose formal procedure is described in [this paper](https://arxiv.org/abs/1310.4546).\n",
        "\n",
        "\n",
        "* If your embeddings don't improve the downstream task, instead of repeating the training on a very large corpus (e.g., with gigas of tokens), which would take days or even weeks, download a pretrained word-vectors (e.g., [GloVe](https://nlp.stanford.edu/projects/glove/)) and train again the logistic regression with the large movie review dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDHLAVUNen2m"
      },
      "source": [
        "[Back to summary.](#topo)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "GCBouAgYen2S"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}